\documentclass[a4paper]{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2022


% ready for submission
%\usepackage{ofu_xai_2022}

\input{math_commands.tex}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{ofu_xai_2022}


% to compile a camera-ready version, add the [final] option, e.g.:
\usepackage[final, nonatbib]{ofu_xai_2022}

% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{ofu_xai_2022}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{graphicx}

\usepackage[round]{natbib}
%%%%

\title{ForestKNN - Combining Random Forests and KNN\\ {\large xAI-Proj-M: Master Project Explainable Machine Learning }}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{%
  Ali Ostadi\thanks{Degree: M.Sc. WI, matriculation \#: 12345678} \\
  Otto-Friedrich University of Bamberg\\
  96049 Bamberg, Germany\\
  \texttt{ali.ostadi@stud.uni-bamberg.de}\\
  % examples of more authors
   \And
   Andreas Franz Schwab\thanks{Degree: M.Sc. AI, matriculation \#: 2017990}\\
   Otto-Friedrich University of Bamberg\\
   96049 Bamberg, Germany\\
   \texttt{andreas-franz.schwab@stud.uni-bamberg.de} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}


\begin{document}


\maketitle
\def\va{{\bm{a}}}

\begin{abstract}
  In an era where AI systems like ChatGPT consume electricity equivalent to powering 180,000 U.S. households daily \citep{Gordon2024}, efficient machine learning methods are crucial. This project enhances k-Nearest Neighbour (kNN) methods for image classification using vector-databases and latent space analysis. We explored the Cifar10 and Cifar100 \citep{Krizhevsky2009}, as well as the DermaMNIST and BreastMNIST datasets \citep{medmnistv1, medmnistv2}, visualized internal data clusters with tSNE plots and examined the existing neighborhood structures. Furthermore, we implemented a simple kNN in PyTorch and compared its performance to a simple trained Linear Layer.
  Our key contribution, ForestKNN, combines the idea of Random Forests and kNN, and includes multiple kNN classifiers using random subsets of samples and features, improving speed and accuracy over linear probing. Though resource-intensive, ForestKNN shows promise as a robust alternative for image classification. For detailed implementation and results, visit our \href{https://github.com/git2andi/xAI_Proj_M}{Git Repository}.
  
\end{abstract}


\section{Introduction}\label{introduction}
The emergence of deep learning has led to unprecedented advances in various fields, including medical image analysis. This project seeks to explore the fundamental principles of deep learning and to leverage its potential in a practical setting. Our investigation is divided into three parts: understanding vector-databases and latent spaces, evaluating performance with traditional and advanced methods, and developing an enhanced kNN algorithm.

Image classification is a cornerstone of many AI applications, such as disease diagnosis, autonomous vehicles, and security systems. Traditional methods like k-Nearest Neighbour (kNN) offer simplicity and interpretability but struggle with high-dimensional data and large datasets. Improving these methods to handle modern, complex datasets is crucial for advancing practical AI applications.

At the centre of our project is the hypothesis that specific adaptations and refinements of deep learning techniques can significantly improve model performance on both simple and complex classification tasks
\dots

Project structures
\dots


\subsection{Related Work}
kNN has long been a popular choice due to its simplicity and ease of understanding. However, its performance can degrade with the increase in data dimensionality and size. Linear Probing, which involves training a linear classifier on top of pre-trained features, has been suggested as a means to improve kNN performance \citep{Alain2016}. Additionally, ensemble methods like Random Forests enhance model robustness and accuracy by combining multiple classifiers \citep{Breiman2001}. Research has also focused on latent space exploration to gain insights into data structures and relationships. Techniques like tSNE are used to visualize these latent spaces, revealing patterns and class overlaps that inform the development of more sophisticated models \citep{Maaten2008}.

\subsection{Contribution}
This project aims to enhance kNN for image classification by integrating ensemble learning principles. We start by exploring vector-databases and latent spaces of several image datasets, including Cifar10, Cifar100, DermaMNIST, and BreastMNIST.\@ By visualizing data distributions and examining neighborhood structures, we gain insights that guide our model development.

In the second phase, we implement a simple kNN in PyTorch and compare its performance against a Linear Layer trained with only a single layer. This comparison helps establish a baseline for further improvements.

Our key contribution, ForestKNN, combines the concepts of Random Forests and kNN. ForestKNN uses multiple kNN classifiers on random subsets of samples and features, aggregating their results through majority voting. This approach enhances accuracy, and once the optimal combination of parameters and methods is identified, it also reduces computation time. 


\section{Methods}\label{methods}
\subsection{Simple kNN}
To evaluate the performance of the k-Nearest Neighbour (kNN) classifier, we implemented a custom kNN model using PyTorch, enabling the use of GPU acceleration. The kNN algorithm classifies a data point by identifying the k closest points in the training set and assigning the majority class among these neighbors. The kNN classifier is initialized with the number of neighbors k and the device (CPU or GPU) on which computations are performed. The training process involves storing the training data and corresponding labels as PyTorch tensors on the specified device. During prediction, the classifier computes pairwise distances between the input data and the training data using the Euclidean distance metric. The nearest neighbors are identified by sorting these distances. The class labels of the nearest neighbors are then used to predict the class of each input sample by taking the mode (most common label) of these labels.

\subsection{Simple Linear Layer}
The Linear Probing method involves training a simple linear classifier using the feature representations (embeddings) from the datasets. This method aims to improve classification by leveraging a single layer neural network directly on the embeddings. The Linear Layer is initialized with the input dimension, representing the feature size of the embeddings, and the number of classes in the dataset. The training process involves defining a loss function (Cross-Entropy Loss) and an optimizer (Adam). The model is trained by performing forward and backward passes to minimize the loss function over a series of epochs. The training data is fed into the linear layer, and the model parameters are updated to improve classification accuracy. Hyperparameter tuning is performed by training the model with various learning rates, batch sizes, and epochs. The performance on the validation set is monitored to identify the best set of hyperparameters.

\subsection{Data Preparation and Evaluation}
For loading the data, we used the given embeddings and labels from the Cifar10, Cifar100, DermaMNIST, and BreastMNIST datasets, stored in numpy files. For datasets without predefined validation sets, the training data was split into training and validation sets (80/20 split) to facilitate model evaluation and hyperparameter tuning. The data was normalized to ensure consistent feature scaling, crucial for the performance of distance-based algorithms like kNN\@. The training and evaluation process for the kNN model involved training the model using the training data and evaluating its performance on the validation set to identify the optimal number of neighbors k. The best k value was then used to evaluate the model on the test set. Performance metrics such as accuracy, precision, recall, and F1 score were calculated to assess the model's effectiveness. For the linear layer, various hyperparameter configurations were tested. The model's performance on the validation set was used to select the best hyperparameters. The final model was then evaluated on the test set, and the performance metrics were recorded.

\section{Datasets}\label{datasets}

\subsection{Cifar10}\label{cifar10}
The CIFAR-10 dataset \citep{Krizhevsky2009} consists of 60,000 32×32 color images in 10 different classes, with 6,000 images per class. These classes include airplanes, cars, birds, cats, deer, dogs, frogs, horses, ships, and trucks. The dataset is split into a training set of 50,000 images and a test set of 10,000 images. Each image is labeled with one of the 10 classes, providing a robust benchmark for evaluating image recognition algorithms. The embeddings provided for this dataset have 768 dimensions. Some examples of the CIFAR-10 dataset can be seen in Figure~\ref{fig:cifar10}.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.5\linewidth]{images/cifar10_images.png}
  \caption{Example images of the CIFAR-10 dataset.}\label{fig:cifar10}
\end{figure}

\subsection{Cifar100}\label{cifar100}
Similar to CIFAR-10, the CIFAR-100 dataset \citep{Krizhevsky2009} contains 60,000 32×32 color images, but it is divided into 100 classes, with 600 images per class. The classes are grouped into 20 superclasses, with each image labeled at two levels of granularity: a coarse label (superclass) and a fine label (class). The dataset is also divided into a training set of 50,000 images and a test set of 10,000 images. CIFAR-100 is more challenging than CIFAR-10 due to its larger number of classes and the finer granularity of the labels. The embeddings provided for this dataset have 768 dimensions. Some examples of the CIFAR-100 dataset can be seen in Figure~\ref{fig:cifar100}.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.5\linewidth]{images/cifar100_images.png}
  \caption{Example images of the CIFAR-10 dataset.}\label{fig:cifar100}
\end{figure}

\subsection{DermaMNIST}\label{DermaMNIST}
The DermMNIST subset of the MedMNIST collection focuses on dermatological image classification, specifically for skin lesion analysis. This dataset includes 10,015 images, each resized to 3×28×28 pixels from their original higher resolution. The images are classified into seven categories: actinic keratoses, basal cell carcinoma, benign keratosis-like lesions, dermatofibroma, melanocytic nevi, melanoma, and vascular lesions. The training set contains 7,007 images, while the test set includes 2,008 images. The complexity of classifying various skin conditions makes DermMNIST a valuable dataset for developing and evaluating medical image classification algorithms. The embeddings provided for this dataset have 768 dimensions. Some examples of the DermMNIST dataset can be seen in Figure~\ref{fig:dermamnist} \citep{medmnistv1, medmnistv2}.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.5\linewidth]{images/dermamnist_images.png}
  \caption{Example images of the CIFAR-10 dataset.}\label{fig:dermamnist}
\end{figure}

\subsection{BreastMNIST}\label{BreastMNIST}
The BreastMNIST subset is another component of the MedMNIST collection, designed for breast ultrasound image classification. It comprises 780 images of breast lesions, categorized into three types: normal, benign, and malignant. Originally, the images were high-resolution, but for the purposes of this subset, they were resized to 28×28 pixels. The dataset is split into a training set of 546 images and a test set of 234 images. BreastMNIST presents a significant challenge due to the subtle visual differences between normal, benign, and malignant lesions, necessitating the use of sophisticated deep learning models. Some examples of the BreastMNIST dataset can be seen in Figure~\ref{fig:breastmnist} \citep{medmnistv1, medmnistv2}.


\begin{figure}[h]
  \centering
  \includegraphics[width=0.5\linewidth]{images/breastmnist_images.png}
  \caption{Example images of the CIFAR-10 dataset.}\label{fig:breastmnist}
\end{figure}



\subsubsection{Headings: third level}


Third-level headings should be in 10-point type.


\paragraph{Paragraphs}


There is also a \verb+\paragraph+ command available, which sets the heading in
bold, flush left, and inline with the text, with the heading followed by 1\,em
of space.


\section{Citations, figures, tables, references}
\label{others}


These instructions apply to everyone.


\subsection{Citations within the text}


The \verb+natbib+ package will be loaded for you by default.  Citations may be
author/year or numeric, as long as you maintain internal consistency.  As to the
format of the references themselves, any style is acceptable as long as it is
used consistently.


The documentation for \verb+natbib+ may be found at
\begin{center}
  \url{http://mirrors.ctan.org/macros/latex/contrib/natbib/natnotes.pdf}
\end{center}
Of note is the command \verb+\citet+, which produces citations appropriate for
use in inline text.  For example,
\begin{verbatim}
   \citet{He_2016_CVPR} investigated\dots
\end{verbatim}
produces
\begin{quote}
  \citet{He_2016_CVPR} investigated\dots
\end{quote}

For standard reference the command \verb+\citep+ is appropriate and produces \citep{He_2016_CVPR}. Multiple references can be cited e.g. with
\begin{verbatim}
    \citep{Bengio_chapter2007,He_2016_CVPR,Hinton06,goodfellow2016deep}
\end{verbatim}
yielding 
\begin{quote}
    \citep{Bengio_chapter2007,He_2016_CVPR,Hinton06,goodfellow2016deep}
\end{quote}

If you wish to load the \verb+natbib+ package with options, you may add the
following before loading the \verb+ofu_xao_2022+ package:
\begin{verbatim}
   \PassOptionsToPackage{options}{natbib}
\end{verbatim}


If \verb+natbib+ clashes with another package you load, you can add the optional
argument \verb+nonatbib+ when loading the style file:
\begin{verbatim}
   \usepackage[nonatbib]{ofu_xao_2022}
\end{verbatim}


\subsection{Footnotes}


Footnotes should be used sparingly.  If you do require a footnote, indicate
footnotes with a number\footnote{Sample of the first footnote.} in the
text. Place the footnotes at the bottom of the page on which they appear.
Precede the footnote with a horizontal rule of 2~inches.


Note that footnotes are properly typeset \emph{after} punctuation
marks.\footnote{As in this example.}


\subsection{Figures}


\begin{figure}
  \centering
  \fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
  \caption{Sample figure caption.}
\end{figure}


All artwork must be neat, clean, and legible. Lines should be dark enough for
purposes of reproduction. The figure number and caption always appear after the
figure. Place one line space before the figure caption and one line space after
the figure. The figure caption should be lower case (except for first word and
proper nouns); figures are numbered consecutively.


You may use color figures.  However, it is best for the figure captions and the
paper body to be legible if the paper is printed in either black/white or in
color.


\subsection{Tables}


All tables must be centered, neat, clean and legible.  The table number and
title always appear before the table.  See Table~\ref{sample-table}.


Place one line space before the table title, one line space after the
table title, and one line space after the table. The table title must
be lower case (except for first word and proper nouns); tables are
numbered consecutively.


Note that publication-quality tables \emph{do not contain vertical rules.} We
strongly suggest the use of the \verb+booktabs+ package, which allows for
typesetting high-quality, professional tables:
\begin{center}
  \url{https://www.ctan.org/pkg/booktabs}
\end{center}
This package was used to typeset Table~\ref{sample-table}.


\begin{table}
  \caption{Sample table title}
  \label{sample-table}
  \centering
  \begin{tabular}{lll}
    \toprule
    \multicolumn{2}{c}{Part}                   \\
    \cmidrule(r){1-2}
    Name     & Description     & Size ($\mu$m) \\
    \midrule
    Dendrite & Input terminal  & $\sim$100     \\
    Axon     & Output terminal & $\sim$10      \\
    Soma     & Cell body       & up to $10^6$  \\
    \bottomrule
  \end{tabular}
\end{table}


\section{Final instructions}


Do not change any aspects of the formatting parameters in the style files.  In
particular, do not modify the width or length of the rectangle the text should
fit into, and do not change font sizes (except perhaps in the
\textbf{References} section; see below). Please note that pages should be
numbered.


\section{Preparing PDF files}


Please prepare submission files with paper size ``A4,'' and not, for
example, ``Letter''.


Fonts can be the main cause of problems. Your PDF file should only
contain Type 1 or Embedded TrueType fonts.



\subsection{Margins in \LaTeX{}}


Most of the margin problems come from figures positioned by hand using
\verb+\special+ or other commands. We suggest using the command
\verb+\includegraphics+ from the \verb+graphicx+ package. Always specify the
figure width as a multiple of the line width as in the example below:
\begin{verbatim}
   \usepackage[pdftex]{graphicx} ...
   \includegraphics[width=0.8\linewidth]{myfile.pdf}
\end{verbatim}
See Section 4.4 in the graphics bundle documentation
(\url{http://mirrors.ctan.org/macros/latex/required/graphics/grfguide.pdf})


A number of width problems arise when \LaTeX{} cannot properly hyphenate a
line. Please give LaTeX hyphenation hints using the \verb+\-+ command when
necessary.


% \begin{ack}
% Use unnumbered first level headings for the acknowledgments. All acknowledgments
% go at the end of the paper before the list of references. Moreover, you are required to declare
% funding (financial activities supporting the submitted work) and competing interests (related financial activities outside the submitted work).
% More information about this disclosure can be found at: \url{https://neurips.cc/Conferences/2022/PaperInformation/FundingDisclosure}.


% Do {\bf not} include this section in the anonymized submission, only in the final paper. You can use the \texttt{ack} environment provided in the style file to autmoatically hide this section in the anonymized submission.
% \end{ack}

\section{Notation}
\input{math_notation.tex}

\section*{References}

Any choice of citation style is acceptable as long as you are
consistent. It is permissible to reduce the font size to \verb+small+ (9 point)
when listing the references.
Note that the Reference section does not count towards the page limit.
\medskip


\bibliography{bibliography}
\bibliographystyle{abbrvnat}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Declaration of Authorship
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section*{Declaration of Authorship}
All final papers have to include the following ‘Declaration of Authorship’:

{\parindent 0cm
%%%%%%%%%%%%%%%%%%%%%%%%%%German%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Declaration of Authorship}
Ich erkläre hiermit gemäß § 9 Abs. 12 APO, dass ich die vorstehende Projektarbeit selbstständig verfasst und keine anderen als die angegebenen Quellen und Hilfsmittel benutzt habe. Des Weiteren erkläre ich, dass die digitale Fassung der gedruckten Ausfertigung der Projektarbeit ausnahmslos in Inhalt und Wortlaut entspricht und zur Kenntnis genommen wurde, dass diese digitale Fassung einer durch Software unterstützten, anonymisierten Prüfung auf Plagiate unterzogen werden kann.\\
\vspace{2\baselineskip}
  
Bamberg, \today

\rule[0.5em]{14em}{0.5pt} \hspace{0.25\linewidth}\rule[0.5em]{14em}{0.5pt}
\vspace{1em}
\hspace{4em} (Place, Date) \hspace{0.51\linewidth} (Signature)

Bamberg, \today

\rule[0.5em]{14em}{0.5pt} \hspace{0.25\linewidth}\rule[0.5em]{14em}{0.5pt}
\vspace{1em}
\hspace{4em} (Place, Date) \hspace{0.51\linewidth} (Signature)

Bamberg, \today

\rule[0.5em]{14em}{0.5pt} \hspace{0.25\linewidth}\rule[0.5em]{14em}{0.5pt}
\vspace{1em}
\hspace{4em} (Place, Date) \hspace{0.51\linewidth} (Signature)
}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\appendix


\section{Appendix}


Optionally include extra information (complete proofs, additional experiments and plots) in the appendix.
This section will often be part of the supplemental material.


\end{document}
