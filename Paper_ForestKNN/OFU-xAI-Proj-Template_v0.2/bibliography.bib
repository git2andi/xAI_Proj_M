# Abstract
@article{Gordon2024,
  author    = {Cindy Gordon},
  title     = {ChatGPT and Generative AI Innovations Are Creating Sustainability Havoc},
  journal   = {Forbes},
  year      = {2024},
  url       = {https://www.forbes.com/sites/cindygordon/2024/03/12/chatgpt-and-generative-ai-innovations-are-creating-sustainability-havoc/},
  note      = {Accessed: 2024-07-27}
}

@article{Krizhevsky2009,
  author    = {Alex Krizhevsky},
  title     = {Learning Multiple Layers of Features from Tiny Images},
  year      = {2009},
  journal   = {Technical Report, University of Toronto},
  url       = {https://www.cs.toronto.edu/~kriz/cifar.html},
  note      = {Accessed: 2024-07-27}
}

#MedMNIST
@article{medmnistv2,
    title={MedMNIST v2-A large-scale lightweight benchmark for 2D and 3D biomedical image classification},
    author={Yang, Jiancheng and Shi, Rui and Wei, Donglai and Liu, Zequan and Zhao, Lin and Ke, Bilian and Pfister, Hanspeter and Ni, Bingbing},
    journal={Scientific Data},
    volume={10},
    number={1},
    pages={41},
    year={2023},
    publisher={Nature Publishing Group UK London}
}

@inproceedings{medmnistv1,
    title={MedMNIST Classification Decathlon: A Lightweight AutoML Benchmark for Medical Image Analysis},
    author={Yang, Jiancheng and Shi, Rui and Ni, Bingbing},
    booktitle={IEEE 18th International Symposium on Biomedical Imaging (ISBI)},
    pages={191--195},
    year={2021}
}

#DermaMNIST
@data{DBW86T_2018,
author = {Tschandl, Philipp},
publisher = {Harvard Dataverse},
title = {{The HAM10000 dataset, a large collection of multi-source dermatoscopic images of common pigmented skin lesions}},
UNF = {UNF:6:KCZFcBLiFE5ObWcTc2ZBOA==},
year = {2018},
version = {V4},
doi = {10.7910/DVN/DBW86T},
url = {https://doi.org/10.7910/DVN/DBW86T}
}



#BreastMNIST
@article{ALDHABYANI2020104863,
title = {Dataset of breast ultrasound images},
journal = {Data in Brief},
volume = {28},
pages = {104863},
year = {2020},
issn = {2352-3409},
doi = {https://doi.org/10.1016/j.dib.2019.104863},
url = {https://www.sciencedirect.com/science/article/pii/S2352340919312181},
author = {Walid Al-Dhabyani and Mohammed Gomaa and Hussien Khaled and Aly Fahmy},
keywords = {Ultrasound, Breast cancer, Medical images, Dataset, Deep learning, Classification, Segmentation, Detection},
}





#Introduction
@article{Alain2016,
  author    = {Guillaume Alain and Yoshua Bengio},
  title     = {Understanding intermediate layers using linear classifier probes},
  journal   = {arXiv preprint arXiv:1610.01644},
  year      = {2016},
  url       = {https://arxiv.org/abs/1610.01644}
}

@book{Breiman2001,
  author    = {Leo Breiman},
  title     = {Random Forests},
  publisher = {Springer},
  year      = {2001},
  pages     = {5--32}
}

@article{Maaten2008,
  author    = {Laurens van der Maaten and Geoffrey Hinton},
  title     = {Visualizing Data using t-SNE},
  journal   = {Journal of Machine Learning Research},
  volume    = {9},
  number    = {Nov},
  year      = {2008},
  pages     = {2579--2605},
  url       = {http://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf}
}

# additional Distances
@article{chebyshev1867,
  title={Théorie des mécanismes connus sous le nom de parallélogrammes},
  author={Chebyshev, P. L.},
  journal={Mathematische Annalen},
  year={1867}
}

@book{minkowski1896,
  title={Geometrie der Zahlen},
  author={Minkowski, H.},
  year={1896},
  publisher={Teubner}
}

@article{mahalanobis1936,
  title={On the generalized distance in statistics},
  author={Mahalanobis, P. C.},
  journal={Proceedings of the National Institute of Sciences of India},
  volume={2},
  number={1},
  pages={49--55},
  year={1936}
}

@article{lance1967,
  title={A general theory of classificatory sorting strategies: 1. Hierarchical systems},
  author={Lance, G. N. and Williams, W. T.},
  journal={The Computer Journal},
  volume={9},
  number={4},
  pages={373--380},
  year={1967},
  publisher={Oxford University Press}
}


# Unsorted
@article{PCA,
	title = {Principal component analysis},
	volume = {2},
	rights = {https://www.elsevier.com/tdm/userlicense/1.0/},
	issn = {01697439},
	url = {https://linkinghub.elsevier.com/retrieve/pii/0169743987800849},
	doi = {10.1016/0169-7439(87)80084-9},
	pages = {37--52},
	number = {1},
	journaltitle = {Chemometrics and Intelligent Laboratory Systems},
	shortjournal = {Chemometrics and Intelligent Laboratory Systems},
	author = {Wold, Svante and Esbensen, Kim and Geladi, Paul},
	urldate = {2024-07-26},
	date = {1987},
        year ={1978},
	langid = {english}
}

@article{Random_Forest,
	title = {Random Forest},
	volume = {45},
	issn = {08856125},
	url = {http://link.springer.com/10.1023/A:1010933404324},
	doi = {10.1023/A:1010933404324},
	pages = {5--32},
	number = {1},
	journaltitle = {Machine Learning},
	author = {Breiman, Leo},
	urldate = {2024},
	year = {2001},
	file = {Full Text:C\:\\Users\\msi\\Zotero\\storage\\ZWLIUCED\\Breiman - 2001 - [No title found].pdf:application/pdf},
}

@incollection{stratified,
	location = {Berlin, Heidelberg},
	title = {Stratified Sampling},
	isbn = {978-3-642-04897-5 978-3-642-04898-2},
	url = {http://link.springer.com/10.1007/978-3-642-04898-2_574},
	pages = {1547--1550},
	booktitle = {International Encyclopedia of Statistical Science},
	publisher = {Springer Berlin Heidelberg},
	author = {Cohen, Michael P.},
	editor = {Lovric, Miodrag},
	urldate = {2024},
	year = {2011},
	langid = {english},
	doi = {10.1007/978-3-642-04898-2_574},
}

@article{smote,
	title = {{SMOTE}: Synthetic Minority Over-sampling Technique},
	rights = {{arXiv}.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1106.1813},
	doi = {10.48550/ARXIV.1106.1813},
	shorttitle = {{SMOTE}},
	abstract = {An approach to the construction of classifiers from imbalanced datasets is described. A dataset is imbalanced if the classification categories are not approximately equally represented. Often real-world data sets are predominately composed of "normal" examples with only a small percentage of "abnormal" or "interesting" examples. It is also the case that the cost of misclassifying an abnormal (interesting) example as a normal example is often much higher than the cost of the reverse error. Under-sampling of the majority (normal) class has been proposed as a good means of increasing the sensitivity of a classifier to the minority class. This paper shows that a combination of our method of over-sampling the minority (abnormal) class and under-sampling the majority (normal) class can achieve better classifier performance (in {ROC} space) than only under-sampling the majority class. This paper also shows that a combination of our method of over-sampling the minority class and under-sampling the majority class can achieve better classifier performance (in {ROC} space) than varying the loss ratios in Ripper or class priors in Naive Bayes. Our method of over-sampling the minority class involves creating synthetic minority class examples. Experiments are performed using C4.5, Ripper and a Naive Bayes classifier. The method is evaluated using the area under the Receiver Operating Characteristic curve ({AUC}) and the {ROC} convex hull strategy.},
	author = {Chawla, N. V. and Bowyer, K. W. and Hall, L. O. and Kegelmeyer, W. P.},
	urldate = {2024},
	year = {2011},
	note = {Publisher: {arXiv}
Version Number: 1},
	keywords = {Artificial Intelligence (cs.{AI}), {FOS}: Computer and information sciences},
}

@misc{randomProjection,
	title = {Random Projection and Its Applications},
	url = {http://arxiv.org/abs/1710.03163},
	abstract = {Random Projection is a foundational research topic that connects a bunch of machine learning algorithms under a similar mathematical basis. It is used to reduce the dimensionality of the dataset by projecting the data points efficiently to a smaller dimensions while preserving the original relative distance between the data points. In this paper, we are intended to explain random projection method, by explaining its mathematical background and foundation, the applications that are currently adopting it, and an overview on its current research perspective.},
	number = {{arXiv}:1710.03163},
	publisher = {{arXiv}},
	author = {Nabil, Mahmoud},
	urldate = {2024-07-26},
	year = {2017},
	eprinttype = {arxiv},
	eprint = {1710.03163 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\msi\\Zotero\\storage\\S3FAT5WA\\Nabil - 2017 - Random Projection and Its Applications.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\msi\\Zotero\\storage\\MARTQNBL\\1710.html:text/html},
}

@misc{practical_Bayesian,
	title = {Practical Bayesian Optimization of Machine Learning Algorithms},
	url = {http://arxiv.org/abs/1206.2944},
	abstract = {Machine learning algorithms frequently require careful tuning of model hyperparameters, regularization terms, and optimization parameters. Unfortunately, this tuning is often a "black art" that requires expert experience, unwritten rules of thumb, or sometimes brute-force search. Much more appealing is the idea of developing automatic approaches which can optimize the performance of a given learning algorithm to the task at hand. In this work, we consider the automatic tuning problem within the framework of Bayesian optimization, in which a learning algorithm's generalization performance is modeled as a sample from a Gaussian process ({GP}). The tractable posterior distribution induced by the {GP} leads to efficient use of the information gathered by previous experiments, enabling optimal choices about what parameters to try next. Here we show how the effects of the Gaussian process prior and the associated inference procedure can have a large impact on the success or failure of Bayesian optimization. We show that thoughtful choices can lead to results that exceed expert-level performance in tuning machine learning algorithms. We also describe new algorithms that take into account the variable cost (duration) of learning experiments and that can leverage the presence of multiple cores for parallel experimentation. We show that these proposed algorithms improve on previous automatic procedures and can reach or surpass human expert-level optimization on a diverse set of contemporary algorithms including latent Dirichlet allocation, structured {SVMs} and convolutional neural networks.},
	number = {{arXiv}:1206.2944},
	publisher = {{arXiv}},
	author = {Snoek, Jasper and Larochelle, Hugo and Adams, Ryan P.},
	urldate = {2024-07-26},
	year = {2012},
	eprinttype = {arxiv},
	eprint = {1206.2944 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\msi\\Zotero\\storage\\UPN6HLLE\\Snoek et al. - 2012 - Practical Bayesian Optimization of Machine Learnin.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\msi\\Zotero\\storage\\W8RNZC3T\\1206.html:text/html},
}

@article{hyperparameter_Bayesian,
	title = {Hyperparameter Optimization for Machine Learning Models Based on Bayesian Optimization},
	url = {https://www.sciencedirect.com/science/article/pii/S1674862X19300047},
	doi = {https://doi.org/10.11989/JEST.1674-862X.80904120},
	journaltitle = {Journal of Electronic Science and Technology},
	author = {{Jia Wu} and {Xiu-Yun Chen} and {Hao Zhang}},
	year = {2019},
}

